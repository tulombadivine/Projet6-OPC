# -*- coding: utf-8 -*-
"""tulomba_divine_1_notebook_pretraitement_feature_112023.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/147N_xP4DbU7XE-cpAVeuItkgM6uX5eSJ

### Imports de base
"""

import pandas as pd
import numpy as np
import seaborn as sns
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import re
from collections import Counter
import matplotlib.pyplot as plt
import string

"""### Lecture du jeu de données"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
import io

# Charger le fichier CSV dans un DataFrame
df = pd.read_csv(io.StringIO(uploaded['flipkart_com-ecommerce_sample_1050.csv'].decode('utf-8')))

"""### Analyse rapide du jeu de données

- Afficher les 5 premieres lignes du df
"""

df.head()

df.shape

"""- Lister les colonnes"""

df.columns

# Créer un dataframe pour stocker les informations
df_info = pd.DataFrame(columns=['colonne', 'valeurs manquantes', 'pourcentage'])

# Boucle sur les colonnes pour ajouter les informations
for col in df.columns:
    # Nombre de valeurs manquantes
    missing_values = df[col].isnull().sum()
    # Pourcentage de valeurs manquantes
    percentage = (missing_values / df.shape[0]) * 100
    # Ajouter les informations à un nouveau dataframe
    df_info = pd.concat([df_info, pd.DataFrame({'colonne': [col], 'valeurs manquantes': [missing_values], 'pourcentage': [percentage]})], ignore_index=True)

# Trier les colonnes par pourcentage de valeurs manquantes dans l'ordre décroissant
df_info = df_info.sort_values(by='pourcentage', ascending=False)

# Afficher le nom des colonnes et le pourcentage de valeurs manquantes
df_info

# affiche les données présentes en noir et manquantes en beige
plt.figure(figsize=(20,10))
sns.heatmap(df.isna(), cbar=False)

# Compter le nombre d'occurrences de chaque catégorie
category_counts = df['product_category_tree'].value_counts().reset_index()
category_counts = category_counts.head(15)  # Sélectionner les 15 catégories les plus fréquentes

# Tracer le diagramme en barres
plt.figure(figsize=(10, 6))
sns.barplot(data=category_counts, x='product_category_tree', y='index')
plt.title('Fréquence des 15 catégories les plus fréquentes')
plt.xlabel('Fréquence')
plt.ylabel('Catégorie')
plt.show()

"""# Faisabilité de classification – Texte

## Analyse et Nettoyage des features textuelles

### Tokenisation d'une phrase:

La tokenisation est le processus de décomposition du texte en unités plus petites, généralement des mots ou des phrases, appelées "tokens". Cela permet au texte d'être traité et analysé à un niveau plus détaillé.
"""

import nltk
nltk.download('punkt')

from nltk.tokenize import word_tokenize

def tokenize_sentence(sentence):
    """
    Tokenise la phrase donnée en mots individuels.

    Args:
        sentence (str): La phrase à tokeniser.

    Returns:
        list: Liste des tokens (mots) dans la phrase.
    """
    return word_tokenize(sentence)

# Exemple d'utilisation
example_sentence = "Le chat dort."
tokenized_sentence = tokenize_sentence(example_sentence)
tokenized_sentence

"""### Stemming d'une phrase:

Le stemming est le processus de réduction d'un mot à sa forme de base ou à sa racine. Cela facilite l'analyse de texte
"""

from nltk.stem import PorterStemmer

def stem_sentence(sentence):
    """
    Stemme la phrase donnée en réduisant les mots à leur forme de base.

    Args:
        sentence (str): La phrase à stemmer.

    Returns:
        str: Phrase stemmée.
    """
    stemmer = PorterStemmer()
    tokens = word_tokenize(sentence)  # Utilisation de la tokenisation NLTK
    stemmed_words = [stemmer.stem(word) for word in tokens]
    return " ".join(stemmed_words)

# Exemple d'utilisation
example_sentence = "The cats are running and jumping."
stemmed_sentence = stem_sentence(example_sentence)
stemmed_sentence

"""### Lemmatisation, Nettoyage des champs de texte:

La lemmatisation est similaire au stemming et consiste a ne garder que les sens des mots
"""

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()


# Redéfinition de la fonction de nettoyage pour intégrer la tokenisation et le stemming
def clean_text(text):
    # Tokenisation
    tokens = tokenize_sentence(text)

    # Stemming
    stemmer = PorterStemmer()
    stemmed_words = [stemmer.stem(word) for word in tokens]

    # Lemmatisation et nettoyage supplémentaire
    lemmatizer = WordNetLemmatizer()
    cleaned_text = ' '.join([lemmatizer.lemmatize(word) for word in stemmed_words if word not in stop_words])

    # Suppression de la ponctuation restante et conversion en minuscules
    cleaned_text = re.sub(r'[^\w\s]', '', cleaned_text).lower().strip()

    return cleaned_text

"""- **Application de la fonction de nettoyage**"""

nltk.download('wordnet')

# Application de la fonction de nettoyage à la colonne de description
df['cleaned_description'] = df['description'].apply(clean_text)
# Application de la fonction de nettoyage à la colonne de nom du produit
df['cleaned_product_name'] = df['product_name'].apply(clean_text)

# Affichage des résultats
df[['description', 'cleaned_description','product_name','cleaned_product_name']].head()

"""### Avant le nettoyage

"""

# Avant le nettoyage
df['description_length'] = df['description'].apply(lambda x: len(str(x)))
df['description_capitals'] = df['description'].apply(lambda x: sum(1 for c in str(x) if c.isupper()))
df['description_punctuation'] = df['description'].apply(lambda x: sum(1 for c in str(x) if c in string.punctuation))

# Afficher les statistiques
df[['description_length', 'description_capitals', 'description_punctuation']].describe()

# Avant le nettoyage
words = df['description'].apply(lambda x: str(x).split())
all_words = [item for sublist in words for item in sublist]
counter = Counter(all_words)
most_common = counter.most_common(10)
words, counts = zip(*most_common)

fig, ax = plt.subplots()
ax.bar(words, counts)
plt.xticks(rotation=45)
plt.title('Mots les plus communs - Avant le nettoyage')
plt.show()

"""- On constate que ce sont beaucoup de stopwords

### Après le nettoyage
"""

# Après le nettoyage
df['cleaned_text'] = df['description'].apply(clean_text)

df['cleaned_text_length'] = df['cleaned_text'].apply(lambda x: len(str(x)))
df['cleaned_text_capitals'] = df['cleaned_text'].apply(lambda x: sum(1 for c in str(x) if c.isupper()))
df['cleaned_text_punctuation'] = df['cleaned_text'].apply(lambda x: sum(1 for c in str(x) if c in string.punctuation))

# Afficher les statistiques
df[['cleaned_text_length', 'cleaned_text_capitals', 'cleaned_text_punctuation']].describe()

# Après le nettoyage
words = df['cleaned_text'].apply(lambda x: str(x).split())
all_words = [item for sublist in words for item in sublist]
counter = Counter(all_words)
most_common = counter.most_common(10)
words, counts = zip(*most_common)

fig, ax = plt.subplots()
ax.bar(words, counts)
plt.xticks(rotation=45)
plt.title('Mots les plus communs - Après le nettoyage')
plt.show()

"""- Il n'ya plus de stopwords, mais uniquement des mots liés aux descriptions des biens"""

from wordcloud import WordCloud

# Histogrammes Seaborn
fig, axes = plt.subplots(3, 2, figsize=(10, 15))

sns.histplot(df['description_length'], bins=30, ax=axes[0, 0], color='blue').set_title('Longueur des descriptions - Avant nettoyage')
sns.histplot(df['cleaned_text_length'], bins=30, ax=axes[0, 1], color='green').set_title('Longueur des descriptions - Après nettoyage')

sns.histplot(df['description_capitals'], bins=30, ax=axes[1, 0], color='blue').set_title('Nombre de majuscules - Avant nettoyage')
sns.histplot(df['cleaned_text_capitals'], bins=30, ax=axes[1, 1], color='green').set_title('Nombre de majuscules - Après nettoyage')

sns.histplot(df['description_punctuation'], bins=30, ax=axes[2, 0], color='blue').set_title('Nombre de signes de ponctuation - Avant nettoyage')
sns.histplot(df['cleaned_text_punctuation'], bins=30, ax=axes[2, 1], color='green').set_title('Nombre de signes de ponctuation - Après nettoyage')

plt.tight_layout()
plt.show()

"""# Bag-of-Words

Passons à la construction des features en utilisant la méthode bag-of-words (sac de mots). Le bag-of-words est une représentation de texte qui décrit la présence de mots dans un document. Il implique deux choses:

- Un vocabulaire de mots connus.
- Une mesure de la présence de ces mots.


Il existe plusieurs façons de mettre en œuvre le bag-of-words, et nous allons explorer quelques-unes d'entre elles ici, en suivant les recommandations du cahier des charges.

Nous commencerons par utiliser la technique de comptage simple de mots en utilisant CountVectorizer de scikit-learn, puis nous utiliserons la technique Tf-idf (fréquence de terme - fréquence inverse de document) en utilisant TfidfVectorizer.

### Approche 2: Fit et Transform sur « product_name » + « description »

Dans cette approche, nous allons combiner les colonnes "cleaned_product_name" et "cleaned_description" en une seule chaîne pour chaque produit, puis ajuster et transformer les données en utilisant cette combinaison.
"""

# Combinaison des colonnes "cleaned_product_name" et "cleaned_description"
df['combined_text'] = df['cleaned_product_name'] + ' ' + df['cleaned_description']

# Utilisation de la colonne "combined_text" pour ajuster et transformer avec TfidfVectorizer
combined_tfidf_features = tfidf_vectorizer.fit_transform(df['combined_text'])

# Conversion en DataFrame pour une meilleure visualisation
combined_tfidf_features_df = pd.DataFrame(combined_tfidf_features.toarray(), columns=tfidf_vectorizer.get_feature_names_out())

# Affichage des premières lignes du DataFrame des features
combined_tfidf_features_df.head()

"""Chaque colonne représente un mot unique dans le vocabulaire, et chaque valeur est le poids Tf-idf correspondant.

### Application Approche 2:
"""

# Création d'un objet PCA
pca_2 = PCA(n_components=0.99)

# Application de l'ACP sur les features TF-IDF de la description
description_pca_features_2 = pca_2.fit_transform(combined_tfidf_features_df)

# Les features réduites sont maintenant stockées dans "description_pca_features_count"

from sklearn.manifold import TSNE

# Création d'un objet TSNE
tsne_2 = TSNE(n_components=2)

# Application de TSNE sur les features PCA
description_tsne_features_2 = tsne_2.fit_transform(description_pca_features_2)

# Conversion en DataFrame pour une meilleure manipulation
description_tsne_features_df_2 = pd.DataFrame(description_tsne_features_2, columns=['Component 1', 'Component 2'])

# Ajout de la catégorie réelle au DataFrame
description_tsne_features_df_2['Category'] = df['product_category_tree'].apply(lambda x: x.split('>>')[0][2:-1]) #
# Tracer le graphique
plt.figure(figsize=(12, 8))
sns.scatterplot(x='Component 1', y='Component 2', hue='Category', data=description_tsne_features_df_2, palette='Set2')
plt.title('Visualisation T-SNE des produits')
plt.show()

from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score
from sklearn.preprocessing import LabelEncoder

# Encodage des étiquettes de catégorie
label_encoder = LabelEncoder()
true_labels = label_encoder.fit_transform(df['product_category_tree'].apply(lambda x: x.split('>>')[0][2:-1]))

# Sélectionne uniquement les composantes T-SNE
tsne_features_for_clustering = description_tsne_features_df_2[['Component 1', 'Component 2']]

# Clustering k-means sur les composantes T-SNE
kmeans = KMeans(n_clusters=7) # catégories
clusters = kmeans.fit_predict(tsne_features_for_clustering)

# Calcul de l'Adjusted Rand Score
ari_score = adjusted_rand_score(true_labels, clusters)


# Calcul de l'Adjusted Rand Score
ari_score = adjusted_rand_score(true_labels, clusters)
print(f"Adjusted Rand Score: {ari_score}")

# Ajouter les clusters au DataFrame
description_tsne_features_df_2['Cluster'] = clusters

plt.figure(figsize=(12, 8))
sns.scatterplot(x='Component 1', y='Component 2', hue='Cluster', data=description_tsne_features_df_2, palette='Set2', legend="full")
plt.title('Visualisation T-SNE avec clustering k-means')
plt.show()

"""# Faisabilité de classification – Text Embedding

Le Text Embedding permet de transformer du text brut non structuré et difficile à traiter par les modeles d'IA, en représentations structurées qui peuvent être traitées plus efficacement. Testons 3 de ces outils :

# WORD2VEC

Word2Vec génère des vecteurs de mots de telle sorte que les mots qui ont des contextes similaires (c'est-à-dire qui apparaissent dans des phrases similaires) ont des vecteurs proches dans l'espace vectoriel. Cela permet d'effectuer des opérations sémantiquement significatives sur les vecteurs, comme "roi" - "homme" + "femme" ≈ "reine".

Commençons par la première étape: tokeniser les descriptions nettoyées.
"""

from nltk.tokenize import word_tokenize

# Tokeniser les descriptions nettoyées
tokenized_descriptions = df['combined_text'].apply(word_tokenize)

from gensim.models import Word2Vec

# Créer un modèle Word2Vec
model_w2v = Word2Vec(sentences=tokenized_descriptions, vector_size=100, window=5, min_count=1, workers=4)

# Entraîner le modèle
model_w2v.train(tokenized_descriptions, total_examples=model_w2v.corpus_count, epochs=10)

# Fonction pour calculer la moyenne des vecteurs de mots pour une description
def average_word_vectors(words, model, vocabulary, num_features):
    feature_vector = np.zeros((num_features,), dtype="float64")
    nwords = 0.
    for word in words:
        if word in vocabulary:
            nwords = nwords + 1.
            feature_vector = np.add(feature_vector, model.wv[word])
    if nwords:
        feature_vector = np.divide(feature_vector, nwords)
    return feature_vector

# Calculer la moyenne des vecteurs de mots pour toutes les descriptions
w2v_feature_array = np.zeros((len(tokenized_descriptions), 100), dtype="float64")
for i, words in enumerate(tokenized_descriptions):
    w2v_feature_array[i, :] = average_word_vectors(words, model_w2v, model_w2v.wv.index_to_key, 100)

# Convertir en DataFrame pour une meilleure manipulation
w2v_features_df = pd.DataFrame(w2v_feature_array)

from sklearn.manifold import TSNE

#******************** FAIRE UN PCA AVANT LE TSNE ******************** + Tfidf

# Création d'un objet TSNE
tsne_word2vec = TSNE(n_components=2)

# Application de TSNE sur les vecteurs Word2Vec (en utilisant w2v_features_df au lieu de word2vec_features)
word2vec_tsne_features = tsne_word2vec.fit_transform(w2v_features_df)

# Conversion en DataFrame
word2vec_tsne_features_df = pd.DataFrame(word2vec_tsne_features, columns=['Component 1', 'Component 2'])

# Ajout de la catégorie réelle au DataFrame
word2vec_tsne_features_df['Category'] = df['product_category_tree'].apply(lambda x: x.split('>>')[0][2:-1])

# Tracer le graphique
plt.figure(figsize=(12, 8))
sns.scatterplot(x='Component 1', y='Component 2', hue='Category', data=word2vec_tsne_features_df, palette='Set2')
plt.title('Visualisation T-SNE des produits (Word2Vec)')
plt.show()

from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score

# Clustering k-means
kmeans_word2vec = KMeans(n_clusters=7)
clusters_word2vec = kmeans_word2vec.fit_predict(word2vec_tsne_features_df[['Component 1', 'Component 2']])

# Calcul de l'Adjusted Rand Score
ari_score_word2vec = adjusted_rand_score(true_labels, clusters_word2vec)
print(f"Adjusted Rand Score (Word2Vec): {ari_score_word2vec}")

"""**Ordre de grandeur de 0.3, la classification est faisable.**"""

# Ajout des clusters K-Means au DataFrame
word2vec_tsne_features_df['Cluster'] = clusters_word2vec

# Création d'une figure pour les deux graphiques
fig, axes = plt.subplots(1, 2, figsize=(18, 8))

# Tracer le graphique T-SNE avec la catégorie réelle
sns.scatterplot(x='Component 1', y='Component 2', hue='Category', data=word2vec_tsne_features_df, palette='Set2', ax=axes[0])
axes[0].set_title('Visualisation T-SNE des produits (Word2Vec)')

# Tracer le graphique T-SNE avec les clusters K-Means
sns.scatterplot(x='Component 1', y='Component 2', hue='Cluster', data=word2vec_tsne_features_df, palette='tab10', ax=axes[1])
axes[1].set_title('Clusters K-Means des produits (Word2Vec)')

plt.show()

"""## BERT

BERT est un modèle de langage pré-entraîné qui utilise des transformateurs pour générer des embeddings de mots dans le contexte de leur phrase complète. Contrairement à Word2Vec, qui ne prend en compte que le contexte local d'un mot, BERT prend en compte l'ensemble du contexte bidirectionnel du mot (c'est-à-dire les mots qui apparaissent avant et après le mot donné). Cela permet à BERT de mieux comprendre le sens d'un mot dans son contexte, ce qui est particulièrement utile pour les mots qui ont plusieurs significations en fonction du contexte.
"""

!pip install transformers

from transformers import BertTokenizer, TFBertModel

# Charge le tokenizer et le modèle BERT
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_layer = TFBertModel.from_pretrained('bert-base-uncased')

combined_texts = df['combined_text'].tolist()
input_tokens = tokenizer(combined_texts, padding=True, truncation=True, return_tensors="tf", max_length=64)

input_dict = {
    'input_ids': input_tokens['input_ids'],
    'attention_mask': input_tokens['attention_mask'],
    'token_type_ids': input_tokens['token_type_ids']
}

bert_outputs = bert_layer(**input_dict)

# Utilise la première dimension de la sortie (représentation [CLS]) comme caractéristiques
bert_features = bert_outputs['last_hidden_state'][:, 0, :]

from sklearn.manifold import TSNE

tsne = TSNE(n_components=2)
tsne_features = tsne.fit_transform(bert_features)

tsne_features_df = pd.DataFrame(tsne_features, columns=['Component 1', 'Component 2'])
tsne_features_df['Category'] = df['product_category_tree'].apply(lambda x: x.split('>>')[0][2:-1])

from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score
from sklearn.preprocessing import LabelEncoder

kmeans = KMeans(n_clusters=7)
clusters = kmeans.fit_predict(tsne_features)

label_encoder = LabelEncoder()
true_labels = label_encoder.fit_transform(df['product_category_tree'].apply(lambda x: x.split('>>')[0][2:-1]))

ari_score = adjusted_rand_score(true_labels, clusters)
print(f"Adjusted Rand Score: {ari_score}")

"""**Ordre de grandeur de 0.3, la classification est faisable.**"""

# Ajout de la catégorie réelle et des clusters au DataFrame
tsne_features_df['Cluster'] = clusters

# Création d'une figure pour les deux graphiques
fig, axes = plt.subplots(1, 2, figsize=(18, 8))

# Tracer le graphique T-SNE avec la catégorie réelle
sns.scatterplot(x='Component 1', y='Component 2', hue='Category', data=tsne_features_df, palette='Set2', ax=axes[0])
axes[0].set_title('Visualisation T-SNE des produits')

# Tracer le graphique T-SNE avec les clusters K-Means
sns.scatterplot(x='Component 1', y='Component 2', hue='Cluster', data=tsne_features_df, palette='tab10', ax=axes[1])
axes[1].set_title('Clusters K-Means des produits')

plt.show()

"""## USE

C'est un modèle qui génère des embeddings pour des phrases entières, plutôt que pour des mots individuels, par conséquent, la signification d'une description peut être capturée globalement.
"""

import tensorflow_hub as hub

# Charger le modèle USE
use_model_url = "https://tfhub.dev/google/universal-sentence-encoder/4"
use_model = hub.load(use_model_url)

# Obtenir les textes à encoder
texts_to_encode = df['combined_text'].tolist()

# Encoder les textes
use_embeddings = use_model(texts_to_encode)
use_features = np.array(use_embeddings)

from sklearn.manifold import TSNE

## AJOUTER PCAAAAA

# Créer un objet TSNE
tsne_use = TSNE(n_components=2)

# Appliquer TSNE sur les vecteurs USE
use_tsne_features = tsne_use.fit_transform(use_features)

# Convertir en DataFrame
use_tsne_features_df = pd.DataFrame(use_tsne_features, columns=['Component 1', 'Component 2'])

# Ajout de la catégorie réelle au DataFrame
use_tsne_features_df['Category'] = df['product_category_tree'].apply(lambda x: x.split('>>')[0][2:-1])

# Tracer le graphique T-SNE
plt.figure(figsize=(12, 8))
sns.scatterplot(x='Component 1', y='Component 2', hue='Category', data=use_tsne_features_df, palette='Set2')
plt.title('Visualisation T-SNE des produits (USE)')
plt.show()

from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score

# Clustering k-means avec USE
kmeans_use = KMeans(n_clusters=7)
clusters_use = kmeans_use.fit_predict(use_tsne_features_df[['Component 1', 'Component 2']])

# Calcul de l'Adjusted Rand Score
ari_score_use = adjusted_rand_score(true_labels, clusters_use)
print(f"Adjusted Rand Score (USE): {ari_score_use}")

# Tracer le graphique T-SNE avec les clusters k-means
use_tsne_features_df['Cluster'] = clusters_use
plt.figure(figsize=(12, 8))
sns.scatterplot(x='Component 1', y='Component 2', hue='Cluster', data=use_tsne_features_df, palette='Set2')
plt.title('Visualisation T-SNE des produits avec clustering k-means (USE)')
plt.show()

"""**Ordre de grandeur de 0.4, la classification est faisable.**

Globalement,  les résultats que nous avons obtenu sont meilleurs que ceux obtenus avec
les techniques plus simples de CountVectorizer ou Tf-idf.
"""

from google.colab import drive
drive.mount('/content/drive')

# enregistrement du df
df.to_csv('/content/drive/My Drive/projet6/clean_df2.csv', index=False)