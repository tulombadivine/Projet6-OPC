# -*- coding: utf-8 -*-
"""tulomba_divine_2_1_notebook_Classification_112023.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xN923xFNE157BvDsj6vSfyQGeBJDiSuX
"""

import numpy as np
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
import os

import csv

from google.colab import drive
drive.mount('/content/drive')

dataframe = pd.read_csv("/content/drive/My Drive/projet6/clean_df2.csv")
df = dataframe.copy()

df.head(4)

df.columns

"""**Analyse d’une image et différentes approches de transformation**"""

import os

# Montez votre Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Définissez le chemin du répertoire contenant vos images
image_dir = "/content/drive/My Drive/projet6/images/"

# Obtenez la liste des fichiers dans ce répertoire
image_files = os.listdir(image_dir)

# Affichez la liste des fichiers (juste pour vérification)
print(image_files)

# Fonction pour calculer et afficher l'histogramme d'une image en niveaux de gris
def plot_histogram(image, title):
    histogram, bin_edges = np.histogram(image, bins=256, range=(0, 255))
    plt.plot(bin_edges[0:-1], histogram)
    plt.title(title)
    plt.xlabel("Intensité de pixel")
    plt.ylabel("Nombre de pixels")
    plt.show()

from PIL import Image
import matplotlib.pyplot as plt

# Chemin vers l'image
image_path = "/content/drive/My Drive/projet6/images/1c0fdd598671f6f4e10b21435d766cf3.jpg"

# Charger l'image
image = Image.open(image_path)

# Afficher l'image
plt.imshow(image)
plt.axis('off') # pour enlever les axes
plt.title('Image Originale')
plt.show()
plot_histogram(image, "Histogramme de l'Image Originale")

"""#Nous allons transformer l'image en niveaux de gris. Cela signifie que l'image sera convertie en une image en noir et blanc, où chaque pixel représente une intensité de gris."""

# Convertir l'image en niveaux de gris
image_gray = image.convert('L')

# Afficher l'image en niveaux de gris
plt.imshow(image_gray, cmap='gray')
plt.axis('off')
plt.title('Image en Niveaux de Gris')
plt.show()
plot_histogram(image_gray, "Histogramme de l'Image")

"""Passons maintenant à la troisième étape, l'égalisation. L'égalisation d'histogramme est une méthode pour ajuster le contraste dans une image en modifiant la distribution des intensités. Cela peut améliorer la visibilité des caractéristiques importantes dans l'image.

Appliquons cette transformation à l'image en niveaux de gris.
"""

from skimage import exposure

# Convertir l'image en niveaux de gris en tableau numpy
image_gray_array = np.array(image_gray)

# Appliquer l'égalisation d'histogramme
image_equalized = exposure.equalize_hist(image_gray_array)

# Afficher l'image égalisée
plt.imshow(image_equalized, cmap='gray')
plt.axis('off')
plt.title('Image avec Égalisation d\'Histogramme')
plt.show()
plot_histogram(image_equalized, "Histogramme de l'Image")

"""- L'égalisation d'histogramme a été appliquée à l'image, ce qui peut aider à améliorer le contraste dans certaines situations. Cette technique est souvent utilisée pour améliorer la visibilité des détails dans les images, en particulier lorsqu'il y a un contraste faible.

### Passons à l'étape suivante, où nous allons appliquer un filtre pour réduire le bruit dans l'image. Le bruit dans une image peut provenir de diverses sources, telles que la compression, la transmission ou la capture. Le filtrage du bruit peut aider à améliorer la qualité de l'image.
"""

from skimage.filters import median
from skimage.morphology import disk

# Appliquer un filtre médian avec un élément structurant en forme de disque
image_denoised = median(image_equalized, disk(5))

# Afficher l'image après le filtrage du bruit
plt.imshow(image_denoised, cmap='gray')
plt.axis('off')
plt.title('Image avec Filtrage du Bruit')
plt.show()
plot_histogram(image_denoised, "Histogramme de l'Image")

"""- Nous avons appliqué un filtre médian à l'image, ce qui peut aider à réduire le bruit tout en préservant les bords et les caractéristiques importantes

La prochaine étape consiste à ajuster le contraste de l'image. Le contraste c'est   la différence dans la luminance ou la couleur qui rend un objet dans une image discernable. L'ajustement du contraste peut aider à rendre l'image plus claire et à améliorer la perception visuelle des caractéristiques.
"""

from skimage import exposure

# Ajuster le contraste de l'image en utilisant un étirement de contraste (1% - 99%)
image_contrast_stretched = exposure.rescale_intensity(image_denoised, in_range=(np.percentile(image_denoised, 1), np.percentile(image_denoised, 99)))

# Afficher l'image avec contraste ajusté
plt.imshow(image_contrast_stretched, cmap='gray')
plt.axis('off')
plt.title('Image avec Contraste Ajusté')
plt.show()
plot_histogram(image_contrast_stretched, "Histogramme de l'Image")

"""- Nous avons ajusté le contraste de l'image en étirant les intensités entre le 1er et le 99e percentiles. Cela a pour effet d'augmenter le contraste global de l'image en éliminant les valeurs extrêmes. Cette transformation peut aider à mettre en évidence les caractéristiques importantes et à améliorer l'aspect visuel de l'image.

Enfin, nous allons appliquer un floutage à l'image. Le floutage est une technique qui réduit les détails dans une image en utilisant un filtre qui prend la moyenne des pixels voisins. Cela peut être utile pour éliminer les détails non pertinents et mettre en évidence les structures plus grandes.
"""

from skimage.filters import gaussian

# Appliquer un floutage gaussien avec un écart type de 2
image_blurred = gaussian(image_contrast_stretched, sigma=2)

# Afficher l'image floutée
plt.imshow(image_blurred, cmap='gray')
plt.axis('off')
plt.title('Image Floutée')
plt.show()
plot_histogram(image_blurred, "Histogramme de l'Image")

"""**Avec ces étapes, nous avons exploré différentes transformations d'image, notamment la conversion en niveaux de gris, l'égalisation d'histogramme, le filtrage du bruit, l'ajustement du contraste et le floutage**"""

import cv2

# Chemin vers l'image
image_path = "/content/drive/My Drive/projet6/images/1c0fdd598671f6f4e10b21435d766cf3.jpg"

# Charger l'image avec OpenCV
image = cv2.imread(image_path, cv2.IMREAD_COLOR)

# Convertir l'image en niveaux de gris
gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# Appliquer une égalisation d'histogramme pour améliorer le contraste
contrast_image = cv2.equalizeHist(gray_image)

# Afficher les images
plt.figure(figsize=(10,5))

plt.subplot(1, 2, 1)
plt.imshow(gray_image, cmap='gray')
plt.title('Image Originale')
plt.axis('off')

plt.subplot(1, 2, 2)
plt.imshow(contrast_image, cmap='gray')
plt.title('Image avec Contraste Amélioré')
plt.axis('off')

plt.show()

from random import choice
import cv2

# Choix aléatoire d'une image dans le répertoire
random_image_file = choice(image_files)
random_image_path = os.path.join(image_dir, random_image_file)

# Chargement de l'image
random_image = cv2.imread(random_image_path)
random_image_gray = cv2.cvtColor(random_image, cv2.COLOR_BGR2GRAY)

# Affichage de l'image
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.imshow(cv2.cvtColor(random_image, cv2.COLOR_BGR2RGB))
plt.title("Image originale")
plt.axis('off')

# Calcul et affichage de l'histogramme
plt.subplot(1, 2, 2)
plt.hist(random_image_gray.ravel(), bins=256, range=[0,256])
plt.title("Histogramme de niveaux de gris")
plt.xlabel("Niveaux de gris")
plt.ylabel("Fréquence")

plt.show()

# Retourner le chemin de l'image aléatoire pour référence
random_image_path

"""# Préparation du DF"""

# Extraire la catégorie principale à partir de la colonne 'product_category_tree'
df['label_name'] = df['product_category_tree'].apply(lambda x: x.split('>>')[0][2:-1])
list_labels = ['Home Furnishing', 'Baby Care', 'Watches',
       'Home Decor & Festive Needs', 'Kitchen & Dining',
       'Beauty and Personal Care', 'Computers']

from sklearn.preprocessing import LabelEncoder

# Création d'un objet LabelEncoder
label_encoder = LabelEncoder()

# Transformation des catégories principales en étiquettes numériques
df['label'] = label_encoder.fit_transform(df['label_name'])

# Affichage des premières lignes pour vérifier la nouvelle colonne
df[['label_name', 'label']].head()

# affiche les labels uniques
df['label_name'].unique()

# Utilise la fonction groupby pour regrouper les données par 'label_name' et agréger les 'label' correspondants
grouped_df = df.groupby('label_name')['label'].unique().reset_index()

# Affiche le résultat pour connaître chaque 'label_name' et les 'label' associés
print(grouped_df)

"""**Affichons un exemple des 7 catégories**"""

# Chemin vers le dossier contenant les images
image_dir = "/content/drive/My Drive/projet6/images/"

# Affiche une image aléatoire pour chaque label
plt.figure(figsize=(15, 10))

for idx, label in enumerate(list_labels):
    sample_image_path = df[df['label_name'] == label].sample(1)['image'].values[0]
    sample_image_path = os.path.join(image_dir, sample_image_path)
    sample_image = cv2.imread(sample_image_path)
    sample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)

    plt.subplot(2, 4, idx + 1)
    plt.imshow(sample_image)
    plt.title(label)
    plt.axis('off')

plt.show()

"""##  Faisabilité de classification automatique d’images via SIFT"""

img_ex='/content/drive/My Drive/projet6/images/'

import random

# Sélectionner une image aléatoire
random_image_filename = random.choice(image_files)
image_path = os.path.join(image_dir, random_image_filename)

# Lire l'image en niveaux de gris
image = cv2.imread(image_path, 0)

# Égaliser l'histogramme de l'image
image = cv2.equalizeHist(image)

plt.imshow(image)

# Créer un objet SIFT
sift = cv2.xfeatures2d.SIFT_create()

# Détecter et calculer les descripteurs SIFT
kp, des = sift.detectAndCompute(image, None)

# Dessiner les points clés sur l'image
img_with_keypoints = cv2.drawKeypoints(image, kp, image)

# Afficher l'image avec les points clés
plt.imshow(img_with_keypoints, cmap='gray')
plt.show()

# Afficher les informations sur les descripteurs
print("Descripteurs : ", des.shape)
print()
print(des)

from sklearn.cluster import KMeans

# Chemin vers le dossier contenant les images
image_dir = "/content/drive/My Drive/projet6/images/"

# Créer un objet SIFT
sift = cv2.xfeatures2d.SIFT_create()

"""1. Création des descripteurs de chaque image"""

import time

# Taille désirée pour redimensionner les images
resize_dim = (100, 100)

all_descriptors = []
temps_sift = time.time()

for image_file in image_files:
    image_path = os.path.join(image_dir, image_file)
    image = cv2.imread(image_path, 0)
    # Redimensionner l'image
    image = cv2.resize(image, resize_dim)
    _, des = sift.detectAndCompute(image, None)
    if des is not None:
        all_descriptors.extend(des)

duration_sift = time.time() - temps_sift
print("Temps de traitement SIFT : ", duration_sift, "secondes")

"""2. Création des clusters de descripteurs

"""

from sklearn import cluster

# Estimation du nombre de clusters
k = int(round(np.sqrt(len(all_descriptors)), 0))
print("Nombre de clusters estimés : ", k)

# Clustering avec MiniBatchKMeans
temps_kmeans = time.time()
kmeans = cluster.MiniBatchKMeans(n_clusters=k, init_size=3*k, random_state=0)
kmeans.fit(all_descriptors)
duration_kmeans = time.time() - temps_kmeans
print("Temps de traitement kmeans : ", duration_kmeans, "secondes")

from sklearn.cluster import MiniBatchKMeans
import time

# Nombre de clusters déterminé précédemment
k = 262

# Création de l'objet KMeans
kmeans = MiniBatchKMeans(n_clusters=k, init_size=3*k, random_state=0)

# Mesure du temps de traitement pour le clustering
start_time_kmeans = time.time()

# Convertir all_descriptors en float32
all_descriptors = np.array(all_descriptors, dtype=np.float32)

# Application de KMeans pour regrouper les descripteurs
kmeans.fit(all_descriptors)

# Calcul du temps de traitement
duration_kmeans = time.time() - start_time_kmeans
duration_kmeans

# Fonction pour extraire les caractéristiques BoVW pour une image

def extract_bovw_features(image_file):
    image_path = os.path.join(image_dir, image_file)
    image = cv2.imread(image_path, 0)
    image = cv2.resize(image, resize_dim)
    _, des = sift.detectAndCompute(image, None)
    if des is not None:
        histogram = np.zeros(k)
        des = des.astype(np.float32)  # Convertir les descripteurs en float32
        clusters = kmeans.predict(des)
        for cluster in clusters:
            histogram[cluster] += 1
        return histogram
    else:
        return None

# Extraire les caractéristiques BoVW pour toutes les images
bovw_features = []
filtered_labels = []
for image_file, label in zip(image_files, df['label']):
    feature = extract_bovw_features(image_file)
    if feature is not None:
        bovw_features.append(feature)
        filtered_labels.append(label)

# Convertir en tableau NumPy pour la manipulation ultérieure
bovw_features = np.array(bovw_features)
filtered_labels = np.array(filtered_labels)

bovw_features #pour voir combien des features nous avons

from sklearn.decomposition import PCA

# Ajuster PCA sans spécifier le nombre de composantes
pca = PCA()
pca.fit(bovw_features)

# Calculer la somme cumulée de la variance expliquée
cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)

# Trouver le nombre de composantes nécessaires pour expliquer 95% de la variance
n_components_95 = np.where(cumulative_variance_ratio >= 0.95)[0][0] + 1

print(f"Nombre de composantes nécessaires pour expliquer 95% de la variance : {n_components_95}")

# Ré-appliquer PCA avec le nombre optimal de composantes
pca_95 = PCA(n_components=n_components_95)
bovw_features_reduced_95 = pca_95.fit_transform(bovw_features)

from sklearn.manifold import TSNE

# Appliquer T-SNE
tsne = TSNE(n_components=2)
bovw_features_tsne = tsne.fit_transform(bovw_features_reduced_95)

# Tracer les résultats
plt.scatter(bovw_features_tsne[:, 0], bovw_features_tsne[:, 1], c=filtered_labels, cmap='plasma')
plt.colorbar()
plt.title("catégories réelles")

# Afficher le graphique
plt.show()

"""Il est assez difficile de séparer les images selon le label. Le résultat n’est donc pas très
concluant avec SIFT.
"""

from sklearn.metrics import adjusted_rand_score

# Effectuer le clustering k-means sur les composantes t-SNE
kmeans_tsne = KMeans(n_clusters=7, random_state=0)
clusters_tsne = kmeans_tsne.fit_predict(bovw_features_tsne)

# Affiche les 2 composantes du t-SNE en coloriant selon le numéro de cluster du k-means
plt.scatter(bovw_features_tsne[:, 0], bovw_features_tsne[:, 1], c=clusters_tsne, cmap='plasma')
plt.colorbar()
plt.title("Clusters K-Means sur les Composantes t-SNE")
plt.show()

# Compare la similarité de la catégorisation (catégories réelles / cluster k-means) (ARI)
ari_score = adjusted_rand_score(filtered_labels, clusters_tsne)
print("Adjusted Rand Score (ARI):", ari_score)

"""La
valeur, de l’ordre de 0.00 à 0.1, confirme le visuel.

Analyse par classe

Une matrice de confusion est un outil utilisé pour évaluer les performances d'un modèle de classification
"""

from sklearn.metrics import confusion_matrix, classification_report

# Créer la matrice de confusion
conf_mat = confusion_matrix(filtered_labels, clusters_tsne)


# Nom des catégories
categories = ['Baby Care', 'Beauty and Personal Care', 'Computers',
              'Home Decor & Festive Needs', 'Home Furnishing', 'Kitchen & Dining', 'Watches']

# Convertir la matrice de confusion en pourcentages
conf_mat_percent = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]
conf_mat_percent *= 100

# Visualiser la matrice de confusion en pourcentages
df_cm_percent = pd.DataFrame(conf_mat_percent, index=categories, columns=[str(i) for i in range(7)])
plt.figure(figsize=(10, 7))
sns.heatmap(df_cm_percent, annot=True, cmap="Blues", fmt=".2f")
plt.xlabel("Cluster")
plt.ylabel("Vraie Catégorie")
plt.title("Matrice de Confusion (en pourcentages)")
plt.show()
# Calculer les métriques
print(classification_report(filtered_labels, clusters_tsne))

"""### Ajouter un commentaire

### Interpretation de la matrice de confusion

## Faisabilité de classification automatique d’images via ORB

**Étape 1: Extraction des descripteurs ORB**
- Nous allons créer un objet ORB, puis extraire les descripteurs de toutes les images. En même temps, nous choisirons une image aléatoire pour visualiser les keypoints et les descripteurs.
"""

import cv2
import random

# Créer un objet ORB
orb = cv2.ORB_create()

# Liste pour stocker tous les descripteurs
all_descriptors_orb = []

# Choisir une image aléatoire pour la visualisation
random_image_file = random.choice(image_files)
random_image_path = os.path.join(image_dir, random_image_file)
random_image = cv2.imread(random_image_path, 0)
random_image = cv2.resize(random_image, resize_dim)

# Extraire les descripteurs ORB pour toutes les images
for image_file in image_files:
    image_path = os.path.join(image_dir, image_file)
    image = cv2.imread(image_path, 0)
    image = cv2.resize(image, resize_dim)
    kp, des = orb.detectAndCompute(image, None)

    if des is not None:
        all_descriptors_orb.extend(des)

    # Visualiser les keypoints et descripteurs pour l'image aléatoire
    if image_file == random_image_file:
        image_with_keypoints = cv2.drawKeypoints(random_image, kp, None, color=(0, 255, 0))
        plt.imshow(image_with_keypoints)
        plt.title('Keypoints ORB pour une image aléatoire')
        plt.show()

"""- on utilise ORB pour extraire les descripteurs et stocke tous les descripteurs dans all_descriptors_orb. on peut également visualiser  les keypoints pour une image aléatoire choisie.

Étape 2: Clustering des descripteurs ORB
"""

from sklearn import cluster

# Estimation du nombre de clusters
k_orb = int(round(np.sqrt(len(all_descriptors_orb)), 0))
print("Nombre de clusters estimés pour ORB: ", k_orb)

# Clustering avec MiniBatchKMeans
kmeans_orb = cluster.MiniBatchKMeans(n_clusters=k_orb, init_size=3*k_orb, random_state=0)
kmeans_orb.fit(all_descriptors_orb)

"""Étape 3: Création des caractéristiques BoVW pour ORB

Nous allons maintenant créer les caractéristiques BoVW pour toutes les images en utilisant les clusters ORB.
"""

def extract_bovw_features_orb(image_file):
    image_path = os.path.join(image_dir, image_file)
    image = cv2.imread(image_path, 0)
    image = cv2.resize(image, resize_dim)
    _, des = orb.detectAndCompute(image, None)
    if des is not None:
        histogram = np.zeros(k_orb)
        clusters = kmeans_orb.predict(des)
        for cluster in clusters:
            histogram[cluster] += 1
        return histogram
    else:
        return None

# Extraire les caractéristiques BoVW pour toutes les images et supprimer les images sans descripteurs
bovw_features_orb = []
filtered_labels_orb = []
for image_file, label in zip(image_files, df['label']):
    feature = extract_bovw_features_orb(image_file) # fonction d'extraction ORB
    if feature is not None:
        bovw_features_orb.append(feature)
        filtered_labels_orb.append(label)

# Convertir en tableau NumPy pour la manipulation ultérieure
bovw_features_orb = np.array(bovw_features_orb)
filtered_labels_orb = np.array(filtered_labels_orb)

"""**Étape 4: Visualisation et Analyse**
- Réduction de dimensionnalité avec PCA (95%)
Trouvons le nombre de composants nécessaires pour expliquer 95% de la variance, et appliquons PCA.
"""

pca_orb = PCA(n_components=0.95)  # Conserver 95% de la variance
bovw_features_orb_reduced = pca_orb.fit_transform(bovw_features_orb)

#Visualisation t-SNE (avec les catégories réelles)
tsne_orb = TSNE(n_components=2, random_state=0)
bovw_features_orb_tsne = tsne_orb.fit_transform(bovw_features_orb_reduced)

import matplotlib.pyplot as plt

plt.scatter(bovw_features_orb_tsne[:, 0], bovw_features_orb_tsne[:, 1], c=filtered_labels_orb, cmap='viridis')
plt.colorbar()
plt.title("Visualisation t-SNE des caractéristiques ORB")
plt.show()

kmeans_tsne_orb = KMeans(n_clusters=7, random_state=0)  # 7 catégories
clusters_tsne_orb = kmeans_tsne_orb.fit_predict(bovw_features_orb_tsne)

plt.scatter(bovw_features_orb_tsne[:, 0], bovw_features_orb_tsne[:, 1], c=clusters_tsne_orb, cmap='viridis')
plt.colorbar()
plt.title("Clusters K-Means sur les Composantes t-SNE (ORB)")
plt.show()

ari_score_orb = adjusted_rand_score(filtered_labels_orb, clusters_tsne_orb)
print("Adjusted Rand Score (ARI) avec ORB:", ari_score_orb)

"""Le résultat est similaire au sift"""

from sklearn.metrics import confusion_matrix

# Clustering K-Means
kmeans_orb_tsne = KMeans(n_clusters=7, random_state=0)
clusters_orb_tsne = kmeans_orb_tsne.fit_predict(bovw_features_orb_tsne)

# Calculer la Matrice de Confusion
conf_mat_orb = confusion_matrix(filtered_labels_orb, clusters_orb_tsne)

# Normalisation de la Matrice de Confusion (somme de chaque ligne = 100)
conf_mat_orb_normalized = conf_mat_orb.astype('float') / conf_mat_orb.sum(axis=1)[:, np.newaxis]
conf_mat_orb_normalized = conf_mat_orb_normalized * 100  # Conversion en pourcentage

# Nom des catégories
categories = ['Baby Care', 'Beauty and Personal Care', 'Computers',
              'Home Decor & Festive Needs', 'Home Furnishing', 'Kitchen & Dining', 'Watches']

# Visualiser la Matrice de Confusion Normalisée
df_cm_orb = pd.DataFrame(conf_mat_orb_normalized, index=categories, columns=[str(i) for i in range(7)])
plt.figure(figsize=(10,7))
sns.heatmap(df_cm_orb, annot=True, cmap="Blues", fmt=".2f")
plt.ylabel('Vraies Catégories')
plt.xlabel('Clusters Prédits')
plt.show()

# Calculer les métriques
print(classification_report(filtered_labels_orb, clusters_orb_tsne))

"""Les résultats sont médiocres et similaires au SIFT

Faisabilité de classification automatique d’images via CNN TransferLearning se trouve dans le notebook 2.1

# All pictures: preprocessing, feature extr, classification non supervisée

### 1. Prétraitement des Images :
a. Chargement des Images depuis le Dossier :
"""

# Montez votre Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Définissez le chemin du répertoire contenant vos images
image_dir = "/content/drive/My Drive/projet6/images/"
image_files = os.listdir(image_dir)

# Pour extraire la catégorie principale à partir de `product_category_tree`
df['main_category'] = df['product_category_tree'].apply(lambda x: x.split('>>')[0][2:])

# Pour créer un dictionnaire associant chaque image à sa catégorie principale
image_to_category = pd.Series(df['main_category'].values, index=df['image']).to_dict()

# Pour créer la liste des labels
labels = [image_to_category.get(img, 'Unknown') for img in image_files]

image_to_category

import matplotlib.pyplot as plt
import matplotlib.image as mpimg

# Construire le dictionnaire image_path : label
image_path_to_label_dict = {os.path.join(image_dir, img): cat for img, cat in image_to_category.items()}

# Trouver les catégories uniques
unique_categories = df['main_category'].unique()

# Pour chaque catégorie, afficher 3 images
for category in unique_categories:
    print(category)

    # Sélectionner les images de cette catégorie
    category_images = [img for img, cat in image_path_to_label_dict.items() if cat == category]

    # Si cette catégorie a moins de 3 images, nous prenons toutes les images disponibles
    n_images = min(3, len(category_images))

    # Afficher les n_images premières images
    for i in range(n_images):
        plt.subplot(130 + 1 + i)
        image = mpimg.imread(category_images[i])
        plt.imshow(image)
    plt.show()

#Chemin vers le dossier contenant les images :
image_dir = "/content/drive/My Drive/projet6/images/"
image_files = os.listdir(image_dir)

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.models import Model
from tensorflow.keras.applications import VGG16
import numpy as np
import os

# Charger le modèle VGG16 et retirer la couche de sortie
base_model = VGG16()
model = Model(inputs=base_model.inputs, outputs=base_model.layers[-2].output)

model.summary()

# encodage des labels
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
labels_encoded = le.fit_transform(labels)

import cv2
from keras.applications.vgg16 import preprocess_input

# Charger et prétraiter chaque image individuellement
images = []
for image_file in image_files:
    # Charger l'image
    image = cv2.imread(os.path.join(image_dir, image_file))
    # Redimensionner l'image pour qu'elle corresponde à l'entrée attendue par VGG16
    image = cv2.resize(image, (224, 224))
    # Prétraiter l'image de la même manière que les images sur lesquelles VGG16 a été formé
    image = preprocess_input(image)
    images.append(image)

# Convertir la liste d'images en un tableau numpy
images = np.array(images)

# Utiliser le modèle pour extraire les caractéristiques de chaque image
features = model.predict(images)

# Sauvegarder les caractéristiques dans un fichier
np.save(open('features.npy', 'wb'), features)

from sklearn.decomposition import PCA

# Charger les caractéristiques
features = np.load(open('features.npy', 'rb'))

# Appliquer PCA sur les caractéristiques extraites
pca = PCA(n_components=2)
features_pca = pca.fit_transform(features)



"""# Classification superviée d’images via CNN Transfer Learning"""

# on stocke les labels des catégories dans true_labels
true_labels = df['label_name'].values

# Chemin du répertoire où se trouvent les images
chemin_images = "/content/drive/My Drive/projet6/images/"

"""Création de la fonction dataset_fct : Cette fonction utilise la fonction image_dataset_from_directory de TensorFlow pour créer un ensemble de données à partir d'un répertoire contenant les images. Les images sont chargées, leurs labels sont inférés à partir des noms de sous-répertoires, et elles sont redimensionnées à une taille de (224, 224)"""

import tensorflow as tf

batch_size = 32

def dataset_fct(path, validation_split=0, data_type=None):
    dataset = tf.keras.utils.image_dataset_from_directory(
                    path, labels='inferred', label_mode='categorical',
                    class_names=None, batch_size=batch_size, image_size=(224, 224), shuffle=True, seed=42,
                    validation_split=validation_split, subset=data_type
                    )
    return dataset

"""Diviser les données"""

from sklearn.model_selection import train_test_split

# Séparation en ensembles d'entraînement et de test
train_data, test_data = train_test_split(df, test_size=0.3, random_state=42)

# Séparation de l'ensemble d'entraînement en ensembles d'entraînement et de validation
train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)

"""Créer les dossiers correspondants"""

import shutil

def create_folder_structure(data, folder_name):
    os.makedirs(folder_name, exist_ok=True)
    for index, row in data.iterrows():
        category_folder = os.path.join(folder_name, row['label_name'])
        os.makedirs(category_folder, exist_ok=True)
        source_path = os.path.join(chemin_images, row['image'])
        destination_path = os.path.join(category_folder, row['image'])
        shutil.copyfile(source_path, destination_path)

# Créer les dossiers et copier les images pour chaque ensemble
create_folder_structure(train_data, 'train')
create_folder_structure(val_data, 'validation')
create_folder_structure(test_data, 'test')

"""Préparation des ensembles de données pour l'entraînement et le test"""

dataset_train = dataset_fct('train', validation_split=0.25, data_type='training')
dataset_val = dataset_fct('validation', validation_split=0.25, data_type='validation')
dataset_test = dataset_fct('test', validation_split=0, data_type=None)

"""Définir la fonction pour créer le modèle"""

from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom, Rescaling, GlobalAveragePooling2D, Dense, Dropout
from tensorflow.keras.models import Sequential
from tensorflow.keras.applications import VGG16

def create_model_fct2():
    # Data augmentation
    data_augmentation = Sequential([
        RandomFlip("horizontal", input_shape=(224, 224, 3)),
        RandomRotation(0.1),
        RandomZoom(0.1),
    ])

    # Récupération modèle pré-entraîné
    model_base = VGG16(include_top=False, weights="imagenet", input_shape=(224, 224, 3))
    for layer in model_base.layers:
        layer.trainable = False

    # Définition du nouveau modèle
    model = Sequential([
                data_augmentation,
                Rescaling(1./127.5, offset=-1),
                model_base,
                GlobalAveragePooling2D(),
                Dense(256, activation='relu'),
                Dropout(0.5),
                Dense(7, activation='softmax') # 7 classes dans notre cas
                ])

    # Compilation du modèle
    model.compile(loss="categorical_crossentropy", optimizer='adam', metrics=["accuracy"])

    print(model.summary())

    return model

"""Créer le modèle et définir les callbacks"""

from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping

# Création du modèle
model4 = create_model_fct2()

# Création du callback
model4_save_path = "./model4_best_weights.h5"
checkpoint = ModelCheckpoint(model4_save_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)
callbacks_list = [checkpoint, es]

"""Entraîner le modèle"""

history4 = model4.fit(dataset_train,
                    validation_data=dataset_val,
                    batch_size=32, epochs=50, callbacks=callbacks_list, verbose=1)

# Score du dernier epoch
loss, accuracy = model4.evaluate(dataset_train, verbose=True)
print("Training Accuracy   : {:.4f}".format(accuracy))
loss, accuracy = model4.evaluate(dataset_val, verbose=True)
print("Validation Accuracy :  {:.4f}".format(accuracy))
loss, accuracy = model4.evaluate(dataset_test, verbose=False)
print("Test Accuracy       :  {:.4f}".format(accuracy))

import matplotlib.pyplot as plt

# Récupérer l'historique de l'entraînement
history = history4.history

# Tracer la perte d'entraînement et de validation
plt.plot(history['loss'], label='Training Loss')
plt.plot(history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Tracer l'accuracy d'entraînement et de validation
plt.plot(history['accuracy'], label='Training Accuracy')
plt.plot(history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""En observant les courbes de perte d'entraînement et de validation, on peut constater que l'écart entre elles diminue au fil des époques, ce qui est un signe positif indiquant que notre modèle se généralise de mieux en mieux aux données de validation.

Cependant, après environ 10 époques, la courbe de validation passe légèrement au-dessus de la courbe d'entraînement. Cela pourrait être un signe que le modèle commence à surapprendre les données d'entraînement. Néanmoins, comme l'augmentation de la perte de validation n'est pas significative et reste stable par la suite, il semble que le surapprentissage n'est pas un problème majeur pour notre modèle actuel car il s'est stoppé pour 14 epoques.

En examinant les courbes de précision d'entraînement et de validation, on constate que la précision d'entraînement est supérieure à celle de la validation jusqu'à la 10ème époque. Après cela, les rôles s'inversent mais l'écart entre les deux précisions reste minime.

Cette inversion pourrait signifier que notre modèle a réussi à bien généraliser à partir des données d'entraînement, et est désormais capable de prédire avec plus de précision les données de validation, qu'il n'a jamais vues auparavant.

De plus, le fait que l'écart entre les deux précisions reste minime tout au long de l'entraînement est un signe positif. Cela indique que notre modèle est performant à la fois sur les données d'entraînement et de validation. Ainsi, notre modèle ne semble pas être en surapprentissage ni en sous-apprentissage, ce qui est une bonne nouvelle pour la performance globale de notre modèle.

**Bon apprentissage**

**Epoch optimal** : Epoch 14: early stopping

Test avec Flip, Rotation et Zoom de manière séparée
"""

from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom, Rescaling, GlobalAveragePooling2D, Dense, Dropout
from tensorflow.keras.models import Sequential
from tensorflow.keras.applications import VGG16

import time
import matplotlib.pyplot as plt

# Liste des augmentations de données
data_augmentations_vgg16 = [
    ("Flip", RandomFlip("horizontal", input_shape=(224, 224, 3))),
    ("Rotation", RandomRotation(0.1, input_shape=(224, 224, 3))),
    ("Zoom", RandomZoom(0.1, input_shape=(224, 224, 3))),
]

# Résultats et historiques
results_vgg16 = []
histories_vgg16 = [] # Liste pour stocker les historiques d'entraînement

# Boucle sur chaque augmentation de données
for augmentation_name, augmentation_layer in data_augmentations_vgg16:

    # Modèle avec l'augmentation de données actuelle
    model_vgg16 = Sequential([
                augmentation_layer,
                Rescaling(1./127.5, offset=-1),
                VGG16(include_top=False, weights="imagenet", input_shape=(224, 224, 3)),
                GlobalAveragePooling2D(),
                Dense(256, activation='relu'),
                Dropout(0.5),
                Dense(7, activation='softmax') # 7 classes
                ])

    # Compiler le modèle
    model_vgg16.compile(loss="categorical_crossentropy", optimizer='adam', metrics=["accuracy"])

    # Enregistrer le temps de début
    start_time = time.time()

    # Entraîner le modèle
    history_vgg16 = model_vgg16.fit(dataset_train, validation_data=dataset_val, batch_size=32, epochs=50, callbacks=callbacks_list, verbose=1)

    # Ajouter l'historique à la liste
    histories_vgg16.append(history_vgg16)

    # Calculer le temps de traitement
    processing_time_vgg16 = time.time() - start_time

    # Nombre d'époques où le programme s'est arrêté
    stopped_epoch_vgg16 = len(history_vgg16.history['accuracy'])

    # Évaluer la performance
    train_loss, train_accuracy_vgg16 = model_vgg16.evaluate(dataset_train, verbose=False)
    val_loss, val_accuracy_vgg16 = model_vgg16.evaluate(dataset_val, verbose=False)

    results_vgg16.append((augmentation_name, train_accuracy_vgg16, val_accuracy_vgg16, processing_time_vgg16, stopped_epoch_vgg16))

# Fonction pour tracer les courbes de perte et de précision
def plot_loss_accuracy(histories, augmentation_names):
    for history, aug_name in zip(histories, augmentation_names):
        plt.figure(figsize=(12, 5))

        plt.subplot(1, 2, 1)
        plt.plot(history.history['accuracy'], label='Training Accuracy')
        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
        plt.title(f'{aug_name} - Accuracy')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.legend()

        plt.subplot(1, 2, 2)
        plt.plot(history.history['loss'], label='Training Loss')
        plt.plot(history.history['val_loss'], label='Validation Loss')
        plt.title(f'{aug_name} - Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()

        plt.show()

# Appeler la fonction avec les historiques d'entraînement et les noms des techniques d'augmentation
plot_loss_accuracy(histories_vgg16, [aug[0] for aug in data_augmentations_vgg16])

"""- **Flip** : Sous-apprentissage
- **Rotation**: Sous-apprentissage
- **Zoom**: Sous-apprentissage

**Bilan** : Il vaut mieux éffectuer ces datas augmentations en même temps
"""

# Afficher les résultats
for result in results_vgg16:
    print(f"Augmentation: {result[0]}, Training Accuracy: {result[1]}, Validation Accuracy: {result[2]}, Processing Time: {result[3]} seconds, Stopped Epoch: {result[4]}")

# Créer un DataFrame avec les résultats
df_result_vgg16 = pd.DataFrame(results_vgg16, columns=['Augmentation', 'Training Accuracy', 'Validation Accuracy', 'Processing Time (seconds)', 'Stopped Epoch'])

# Afficher le DataFrame
df_result_vgg16

"""Du temps de traitement plutot long"""

# Graphique pour la précision de l'entraînement
plt.figure(figsize=(10, 6))
sns.barplot(x='Augmentation', y='Training Accuracy', data=df_result_vgg16)
plt.title('Training Accuracy vs Augmentation Techniques')
plt.show()

# Graphique pour la précision de la validation
plt.figure(figsize=(10, 6))
sns.barplot(x='Augmentation', y='Validation Accuracy', data=df_result_vgg16)
plt.title('Validation Accuracy vs Augmentation Techniques')
plt.show()

# Graphique pour le temps de traitement
plt.figure(figsize=(10, 6))
sns.barplot(x='Augmentation', y='Processing Time (seconds)', data=df_result_vgg16)
plt.title('Processing Time vs Augmentation Techniques')
plt.show()

# Graphique pour le nombre d'époques où le programme s'est arrêté
plt.figure(figsize=(10, 6))
sns.barplot(x='Augmentation', y='Stopped Epoch', data=df_result_vgg16)
plt.title('Stopped Epoch vs Augmentation Techniques')
plt.show()

"""## ResNet-50"""

from tensorflow.keras.applications import ResNet50
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout

# Créer une instance de ResNet-50 sans la couche supérieure
base_model = ResNet50(include_top=False, weights="imagenet", input_shape=(224, 224, 3))

# Rendre toutes les couches de base_model non entraînables
for layer in base_model.layers:
    layer.trainable = False

# Définition du nouveau modèle
model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(256, activation='relu'),
    Dropout(0.5),
    Dense(7, activation='softmax')  # 7 est le nombre de classes
])

# Compiler le modèle
model.compile(loss="categorical_crossentropy", optimizer='adam', metrics=["accuracy"])

# Résumé du modèle
model.summary()

# Création du callback
model_save_path = "./resnet50_best_weights.h5"
checkpoint = ModelCheckpoint(model_save_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)
callbacks_list = [checkpoint, es]

# Entraîner le modèle
history_resnet50 = model.fit(dataset_train,
                             validation_data=dataset_val,
                             batch_size=batch_size, epochs=50, callbacks=callbacks_list, verbose=1)

# Évaluer la précision de l'entraînement
loss, accuracy = model.evaluate(dataset_train, verbose=True)
print("Training Accuracy   : {:.4f}".format(accuracy))

# Évaluer la précision de la validation
loss, accuracy = model.evaluate(dataset_val, verbose=True)
print("Validation Accuracy :  {:.4f}".format(accuracy))

# Charger les poids optimaux
model.load_weights(model_save_path)

# Évaluer la précision de validation avec les poids optimaux
loss, accuracy = model.evaluate(dataset_val, verbose=False)
print("Validation Accuracy :  {:.4f}".format(accuracy))

# Évaluer la précision du test avec les poids optimaux
loss, accuracy = model.evaluate(dataset_test, verbose=False)
print("Test Accuracy       :  {:.4f}".format(accuracy))

# Fonction pour tracer les graphiques
def plot_history(history):
    fig, axes = plt.subplots(1, 2, figsize=(15, 5))

    # Plot de la perte
    axes[0].plot(history.history['loss'], label='Train Loss')
    axes[0].plot(history.history['val_loss'], label='Validation Loss')
    axes[0].set_title('Loss vs. Epochs')
    axes[0].set_xlabel('Epochs')
    axes[0].set_ylabel('Loss')
    axes[0].legend()

    # Plot de l'exactitude
    axes[1].plot(history.history['accuracy'], label='Train Accuracy')
    axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy')
    axes[1].set_title('Accuracy vs. Epochs')
    axes[1].set_xlabel('Epochs')
    axes[1].set_ylabel('Accuracy')
    axes[1].legend()

    plt.show()



# Afficher les graphiques pour ResNet-50
plot_history(history_resnet50)

"""Nous constatons un bon apprentissage pour un batch_size de 32, avec une faible perte et une bonne précision  de l'odre de 0.8.

### Optimisation Batch_size + Epoch
"""

batch_sizes = [16, 32, 64, 128]
results = []
histories = [] # Liste pour stocker les historiques de l'entraînement

for batch_size in batch_sizes:
    # Recréer le modèle ResNet-50
    model = create_model_fct2()

    # Définir le callback EarlyStopping
    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)
    callbacks_list = [checkpoint, es]

    # Enregistrer l'heure de début
    start_time = time.time()

    # Entraîner le modèle avec le batch size courant
    history = model.fit(dataset_train, validation_data=dataset_val, batch_size=batch_size, epochs=50,
                        callbacks=callbacks_list, verbose=1)

    # Ajouter l'historique à la liste
    histories.append(history)

    # Enregistrer l'heure de fin
    end_time = time.time()

    # Calculer le temps de traitement
    processing_time = end_time - start_time

    # Évaluer la performance sur l'ensemble d'entraînement
    train_loss, train_accuracy = model.evaluate(dataset_train, verbose=False)

    # Évaluer la performance sur l'ensemble de validation
    val_loss, val_accuracy = model.evaluate(dataset_val, verbose=False)

    # Obtenir le nombre d'époques où l'entraînement s'est arrêté
    stopped_epoch = es.stopped_epoch

    results.append((batch_size, train_accuracy, val_accuracy, processing_time, stopped_epoch))

# Créer un DataFrame à partir des résultats
results_df = pd.DataFrame(results, columns=['Batch Size', 'Training Accuracy', 'Validation Accuracy', 'Processing Time (seconds)', 'Stopped Epoch'])

# Afficher le DataFrame
print(results_df)

results_df

"""En observant les données du tableau, nous constatons que **le batch_size n'a pas d'impact majeur sur la précision de l'entraînement et de la validation**, toutes étant similaires dans ces différents contextes. En effet, le modèle semble performant indépendamment de la taille du lot utilisée pour l'entraînement.

**Cependant, le temps de traitement varie légèrement avec la taille du lot**. **Un batch_size de 64 offre le temps de traitement le plus court**, ce qui peut être un avantage dans le cadre de grands ensembles de données ou lorsque le temps de calcul est une contrainte.

De plus, **le nombre d'époques après lesquelles l'entraînement s'arrête est également similaire pour toutes les tailles de lot**, suggérant que la taille du lot n'a pas d'impact majeur sur le surapprentissage ou le sous-apprentissage.

Compte tenu de ces facteurs, et étant donné que la précision de l'entraînement et de la validation est notre priorité, tout en gardant à l'esprit l'efficacité computationnelle, **je recommanderais d'utiliser un batch_size de 64**. Cette taille de lot offre un bon équilibre entre la précision du modèle et le temps de traitement, tout en évitant de surcharger la mémoire du GPU.
"""

def plot_history(batch_size, history):
    plt.figure(figsize=(12, 5))

    # Afficher la perte (loss)
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title(f'Batch Size {batch_size} - Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    # Afficher la précision (accuracy)
    plt.subplot(1, 2, 2)
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title(f'Batch Size {batch_size} - Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.show()

# Utiliser cette fonction pour afficher les graphiques
for batch_size, history in zip(batch_sizes, histories):
    plot_history(batch_size, history)

"""- Globalement, ResNet-50 offre de meilleurs résultat que VGG16, nous constatons à l'aide de ces graphiques, que les pertes sont petites, et que les apprentissages sont bons avec des scores de l'odre de 80%.

Bilan : ResNet-50 \ Batch_sizes = 64 \ Epoch = 19 est le meilleur modèle.
"""